Throughout the project, we have tried using our own implementation of the
algorithms, which has been very slow and difficult to work with. Thus, we have
utilized Stable Baselines 3, which provides a more efficient implementation
of the algorithms.

Futhermore, we have implemented several improvements to the environment, which
have helped the agents learn better. These improvements include using a
different reward function, using a different observation space, and altering
termination conditions.

Continuation of the project could involve further research into the workings
of the algorithms, and how they can be improved. This could involve looking at
the gradients of the policy and the value function, or it could involve
looking at the Q-values of the agent. It could also involve looking at the
hyperparameters of the algorithm, and how they can be optimized.

Overall, the project has been a success, as we have been able to train an agent
to play the game quite well. The agent is able to complete several levels of
the game, and it is able to complete the game somewhat consistently.

The overal progress is the first step towards achieving the goal of creating
an agent that can play against and along-side humans in several games.