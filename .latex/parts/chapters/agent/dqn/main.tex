\gls{dqn} is a reinforcement learning algorithm that combines Q-learning
with deep learning. The algorithm uses a neural network to approximate the
Q-values of the state-action pairs. The Q-values are updated using the Bellman
equation, which is defined as follows:
\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]
\end{equation}

The \hyperref[alg:dqn]{algorithm} uses experience replay to store and sample
transitions from the environment. The transitions are stored in a replay
memory, which is a fixed-size buffer. The algorithm samples a random minibatch
of transitions from the replay memory to update the Q-values. This helps to
break the correlation between the samples and stabilize the training process.

\begin{algorithm}[H]
    \begin{algorithmic}[1]
        \State Initialize replay memory $D$ to capacity $N$
        \State Initialize action-value function $Q$ with random weights $\theta$
        \For{episode $= 1, M$}
            \State Initialize sequence $s_1 = \{x_1\}$ and preprocessed sequence $\phi_1 = \phi(s_1)$
            \For{t $= 1, T$}
                \State With probability $\epsilon$ select a random action $a_t$
                \State otherwise select $a_t = \argmax_a Q(\phi(s_t), a; \theta)$
                \State Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$
                \State Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$
                \State Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in $D$
                \State Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from $D$
                \State Set $y_j = \begin{cases}
                    r_j & \text{if episode terminates at step } j+1 \\
                    r_j + \gamma \max_{a^\prime} Q(\phi_{j+1}, a^\prime; \theta) & \text{otherwise}
                \end{cases}$
                \State Perform a gradient descent step on $(y_j - Q(\phi_j, a_j; \theta))^2$ with respect to the network parameters $\theta$
            \EndFor
        \EndFor
    \end{algorithmic}
    \caption{Deep Q-learning}
    \label{alg:dqn}
\end{algorithm}

\subsection{Baseline}\label{sec:agent-dqn-baseline}
\input{parts/chapters/agent/dqn/baseline.tex}

\subsection{Prioritized Experience Replay}\label{sec:agent-dqn-per}
\input{parts/chapters/agent/dqn/per/main.tex}

\subsection{Double Deep Q-learning}\label{sec:agent-dqn-ddqn}
\input{parts/chapters/agent/dqn/ddqn.tex}