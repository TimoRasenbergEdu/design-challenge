Much like \gls{dqn}, \gls{ddqn} is a reinforcement learning algorithm that
combines Q-learning with deep learning. The algorithm uses a neural network to
approximate the Q-values of the state-action pairs, and also updates the
Q-values using the Bellman equation, which is defined as follows:
\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a} \hat{Q}(S_{t+1}, a) - Q(S_t, A_t)]    
\end{equation}

The key difference between \gls{dqn} and \gls{ddqn} is that the Q-values are
updated using the maximum Q-value of the next state according to the target
action-value function. This helps to reduce the overestimation of the Q-values
and improve the stability of the training process. This target action-value
function is referred to as $\hat{Q}$, and is updated every $C$ steps to match
the action-value function $Q$. Like \gls{dqn} \hyperref[alg:ddqn]{algorithm}
uses experience replay.

\begin{algorithm}[H]
    \begin{algorithmic}[1]
        \State Initialize replay memory $D$ to capacity $N$
        \State Initialize action-value function $Q$ with random weights $\theta$
        \State Initialize target action-value function $\hat{Q}$ with weights $\theta^- = \theta$
        \For{episode $= 1, M$}
            \State Initialize sequence $s_1 = \{x_1\}$ and preprocessed sequence $\phi_1 = \phi(s_1)$
            \For{t $= 1, T$}
                \State With probability $\epsilon$ select a random action $a_t$
                \State otherwise select $a_t = \argmax_a Q(\phi(s_t), a; \theta)$
                \State Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$
                \State Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$
                \State Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in $D$
                \State Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from $D$
                \State Set $y_j = \begin{cases}
                    r_j & \text{if episode terminates at step } j+1 \\
                    r_j + \gamma \max_{a^\prime} \hat{Q}(\phi_{j+1}, a^\prime; \theta^-) & \text{otherwise}
                \end{cases}$
                \State Perform a gradient descent step on $(y_j - Q(\phi_j, a_j; \theta))^2$ with respect to the network parameters $\theta$
                \State Every $C$ steps reset $\hat{Q} = Q$
            \EndFor
        \EndFor
    \end{algorithmic}
    \caption{Double Deep Q-learning}
    \label{alg:ddqn}
\end{algorithm}