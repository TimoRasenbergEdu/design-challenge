All in all, we have created an agent that is able to play Q*bert quite well.
The agent is able to complete several levels of the game, and it is able to
complete the game somewhat consistently.

Throughout the project, we have delved into a variety of new aspects, such as
\gls{per}, importance sampling, reward clipping, gradient clipping, and
\gls{ppo}. We have also implemented several improvements to the environment,
which have helped the agent learn better. These improvements include using a
different reward function, using a different observation space, and altering
termination conditions. Combined, discovering these new aspects and
implementing these improvements have helped us gain a better understanding of
\gls{rl}, and how it works in practice.
